{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This notebook is for the exploration of Logistic Regression -- it corresponds to Lecture Handout 2\n",
    "#\n",
    "# You will work in the associated Python module, 'lab_2.py'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoload setup (you don't need to edit this cell); instructions to: \n",
    "#   i) enable autoreloading of modules\n",
    "%load_ext autoreload\n",
    "#  ii) import the module 'lab_2' (which will contain your functions) in an autoreloadable way \n",
    "%aimport lab_2\n",
    "# iii) indicate that we want autoreloading to happen on every evaluation.\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "#\n",
    "# 1: Import modules.\n",
    "#\n",
    "##############################################################\n",
    "\n",
    "import pandas as pd  # Pandas is a library for handling structured numerical data.\n",
    "import numpy as np   # NumPy (Numerical Python), high-performance vector / matrix computations.\n",
    "\n",
    "# SciKit-Learn is a module with utility function for data mining & machine learning.\n",
    "import sklearn.linear_model as skl_lm\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score\n",
    "\n",
    "# MatPlotLib provides MATLAB-style plotting utilities.\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['lines.linewidth'] = 1.7\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "#\n",
    "# 2. Loading the ISLR 'Default' dataset\n",
    "#\n",
    "##############################################################\n",
    "\n",
    "# see https://cran.r-project.org/web/packages/ISLR/ISLR.pdf\n",
    "#\n",
    "# This data set contains information on ten thousand bank customers. \n",
    "#\n",
    "# The aim here is to predict which customers will default on their credit card debt.\n",
    "#\n",
    "# The dataset contains 10000 observations on the following 4 variables.\n",
    "#   * 'default': a No/Yes label indicating whether the customer defaulted on their debt\n",
    "#   * 'student': a No/Yes label indicating whether the customer is a student\n",
    "#   * 'balance': the average balance that the customer has remaining on their credit card after making\n",
    "#                their monthly payment\n",
    "#   * 'income' : income of customer\n",
    "\n",
    "# We use the 'pandas' package to read the CSV file into what Pandas calls a 'dataframe' (hence 'df').\n",
    "df = pd.read_csv('Default.csv')\n",
    "\n",
    "# Eyeball the first 10 observations to make sure the data has been loaded OK.  The 'Unnamed' column\n",
    "# is the row number contained in the file.\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "#\n",
    "# 3. Visualise the data\n",
    "#\n",
    "##############################################################\n",
    "\n",
    "# We need to visualise our data\n",
    "# We will ignore the categorical feature 'student' and the row number in the first column.\n",
    "# We'll focus first on the 'balance' and 'income' features.\n",
    "\n",
    "balance = df['balance'].values\n",
    "income = df['income'].values\n",
    "\n",
    "# We'll convert the given outcomes (credit-card default or not) from string to boolean:\n",
    "y = df['default'].values == 'Yes'\n",
    "\n",
    "# We'll plot a subset of the data to keep the graph legible:\n",
    "num_to_plot = 1000;\n",
    "income_subset = income[0:num_to_plot];\n",
    "balance_subset = balance[0:num_to_plot];\n",
    "y_subset = y[0:num_to_plot];\n",
    "\n",
    "# Set up matplotlib figure:\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "ax = plt.gca()\n",
    "\n",
    "# Scatter plot of balance vs income for the 'No Default' class\n",
    "non_defaulters = y_subset == False; # Vector of bool, 'true' means non-defaulter\n",
    "ax.scatter(balance_subset[non_defaulters], income_subset[non_defaulters], s=15, marker='o')\n",
    "\n",
    "# Scatter plot of balance vs income for the 'Default' class\n",
    "defaulters = y_subset == True; # Vector of bool, 'true' means defaulter\n",
    "ax.scatter(balance_subset[defaulters],  income_subset[defaulters],  s=40, marker='+')\n",
    "\n",
    "# Configure plot axes and labels:\n",
    "ax.set_ylim(ymin=0)\n",
    "ax.set_ylabel('Income')\n",
    "ax.set_xlim(xmin=-100)\n",
    "ax.set_xlabel('Balance')\n",
    "ax.legend(['No Default', 'Default'])\n",
    "\n",
    "# Peer into the scatter plot now in search of insight...\n",
    "#\n",
    "# Note that both defaulters and non-defaulters are well-distributed vertically; this \n",
    "# suggests that 'income' is not a great predictive factor.  On the other hand, the \n",
    "# defaulters are mostly in the high-balance region, and the non-defaulters are found\n",
    "# in the low-balance region.  So 'balance' is the better candidate as a feature for class prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having scrutinized the data, we start to implement a predictor (logistic classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: implement cross-entropy (in the lab_2 module)\n",
    "\n",
    "w_test = np.array([0.1, 0.3])\n",
    "X_test = np.array([[1,1], [1,0]])\n",
    "y_test = np.array([0, 1])\n",
    "print(lab_2.cross_entropy(w_test, X_test, y_test))\n",
    "\n",
    "# Should print 0.77870375 (approx) if your function is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: implement gradient computation (in the lab_2 module)\n",
    "w_test = np.array([0.1, 0.3])\n",
    "X_test = np.array([[1,1], [1,0]])\n",
    "y_test = np.array([0, 1])\n",
    "print(lab_2.gradient(w_test, X_test, y_test))\n",
    "# Expected result: [ 0.06183342  0.29934383]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The gradient descent algorithm as explained in page 22.\n",
    "# The greek letter 'eta' in the slide is here denoted 'learning_rate'\n",
    "# The method returns updated weights, and also a vector with the loss function evaluated at every 83rd iteration\n",
    "# (83 was picked just to reduce the amount of data returned).\n",
    "def gradient_descent(w0, X, y, learning_rate, maxit):\n",
    "    w = w0\n",
    "    loss = [];\n",
    "    for i in range(0, maxit):\n",
    "        if (i % 83 == 0):\n",
    "            loss_error = lab_2.cross_entropy(w, X, y)\n",
    "            loss.append(loss_error)\n",
    "      \n",
    "        grad = lab_2.gradient(w,X,y);\n",
    "        w = w - learning_rate * grad;        \n",
    "        i = i + 1\n",
    "        \n",
    "    return w, loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Setting up the Model and Design Matrix\n",
    "#\n",
    "\n",
    "# in this lab we will look at the following model:\n",
    "# y = [ w0 + w1 * balance > 0]\n",
    "#\n",
    "# and ignore the 'income' feature\n",
    "\n",
    "n = balance.shape[0]\n",
    "p = 2\n",
    "\n",
    "X = np.zeros(shape=(n, p))\n",
    "\n",
    "# The first feature is just the value 1 (with associated weight w0); this captures the base rate.\n",
    "X[:,0] = 1;\n",
    "\n",
    "# The second feature is the balance values (with associated weight w1)\n",
    "X[:,1] = balance[:]\n",
    "\n",
    "# Gradient Descent algorithms are quite sensitive to initial conditions\n",
    "# and it is often beneficial to rescale the features to improve the performance. \n",
    "#\n",
    "# In our case we note that balance ranges from 0 to about 2500, which \n",
    "# is much larger in magnitude than the first feature (which is simply 1).\n",
    "# We (arbitrarily) rescale by factor of 1/1000 to make the features be of comparable magnitude.\n",
    "\n",
    "X[:,1] = X[:,1]/1000;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the following we are studying the convergence for 3 different learning rates\n",
    "# This cell might take a while to evaluate!\n",
    "\n",
    "w0 = [0, 0];\n",
    "w, loss = gradient_descent(w0, X, y, learning_rate=50, maxit=10000);\n",
    "print(w)\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,6))\n",
    "ax.plot(loss)\n",
    "\n",
    "w0 = [0, 0];\n",
    "w, loss = gradient_descent(w0, X, y, learning_rate=30, maxit=10000);\n",
    "print(w)\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,6))\n",
    "ax.plot(loss)\n",
    "\n",
    "w0 = [0, 0];\n",
    "w, loss = gradient_descent(w0, X, y, learning_rate=1, maxit=10000);\n",
    "print(w)\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,6))\n",
    "ax.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3: What is the best learning rate value out of 50, 30 and 1?\n",
    "# Look at the graphs above to see.  Return your answer from the 'question_3' function in 'lab_2.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A good way to check that if you reached convergence is to check if the gradient is null (or at least very small):\n",
    "lab_2.gradient(w, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Logistic Regression, we set a parametric model for the likelihood.\n",
    "# We denote logit = x'w and parametrise the likelihood as\n",
    "# p(y_i|logit) = 1/(1 + exp(-logit))\n",
    "#\n",
    "# We want to verify that this is a correct approximation for our problem\n",
    "#\n",
    "# The following function makes an empirical measurement of p(y_i|logit)\n",
    "# by recording in the dataset the proportion of default=True for \n",
    "# a particular logit value (within some small threshold T).\n",
    "def get_empirical_probability(logit, logits_train, y, T=1):\n",
    "    valid = ((logits_train <= logit + T) * (logits_train >= logit - T));    \n",
    "    n_defaults = sum(valid[y==True]);\n",
    "    n_nodefaults = sum(valid[y==False]);\n",
    "    empirical_prob = n_defaults / (n_defaults + n_nodefaults);\n",
    "    return empirical_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Testing set: predict the probability of default for 100 values of balance.\n",
    "\n",
    "n_test = 100\n",
    "X_test = np.zeros(shape=(n_test,2))\n",
    "X_test[0:n_test,0] = 1;\n",
    "X_test[0:n_test,1] = np.linspace(X[:,1].min(), X[:,1].max(), num=n_test)\n",
    "\n",
    "p_test = lab_2.predict(w, X_test)\n",
    "\n",
    "# we compute the logit values and their corresponding empirical probabilities of default\n",
    "logits_test = lab_2.logit(w, X_test)\n",
    "logits_train = lab_2.logit(w, X)\n",
    "p_empirical = [get_empirical_probability(logit, logits_train, y) for logit in logits_test ];\n",
    "\n",
    "# plot the graphs\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,6))\n",
    "\n",
    "ax.scatter(X[y==False,1], y[y==False], alpha= 0.2)\n",
    "ax.scatter(X[y==True,1], y[y==True], alpha= 0.2)\n",
    "ax.plot(X_test[:,1], p_test, color='black')\n",
    "ax.plot(X_test[:,1], p_empirical, ':', color='gray')\n",
    "\n",
    "ax.set_ylabel('Probability of default');\n",
    "ax.set_xlabel('Balance');\n",
    "ax.set_yticks([0, 0.25, 0.5, 0.75, 1.]);\n",
    "ax.legend(['logistic model for probability of default',\n",
    "           'empirically measured probability of default', \n",
    "           'No Default', 'Default'],  prop={'size': 16})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point, we still don't have a classifier. \n",
    "# But all we need is to set a threshold on the predicted probabilities.\n",
    "#\n",
    "# Exercise 4:\n",
    "# Flesh out the function 'predict_class' in the module to give\n",
    "# the predicted class for observations X and weights w.\n",
    "\n",
    "# Use that function here to assess the accuracy of the classifier\n",
    "# for different thresholds.\n",
    "\n",
    "# Accuracy = percentage correctly classified.\n",
    "def accuracy(w, X, y, threshold):\n",
    "    return np.mean(y == lab_2.predict_class(w, X, threshold))\n",
    "\n",
    "w = np.array([-10.63971053,   5.49188453])   # These are good weights!\n",
    "print(accuracy(w, X, y, 0.25))\n",
    "print(accuracy(w, X, y, 0.5))\n",
    "print(accuracy(w, X, y, 0.75))\n",
    "print(accuracy(w, X, y, 0.95))\n",
    "\n",
    "# Update your module function 'question_5' to report the accuracy for a threshold of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}