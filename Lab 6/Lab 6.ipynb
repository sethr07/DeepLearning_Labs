{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b7a3823",
   "metadata": {},
   "source": [
    "# Lab 6: Transfer Learning\n",
    "\n",
    "In the lab, we are going to train an object detection CNN \n",
    "using Transfer Learning. The idea is to start from your \n",
    "lab 5 network and re-use it to speed up the training \n",
    "on a new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae9b3b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "\n",
    "import keras\n",
    "from keras import datasets\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, Activation \n",
    "from keras.layers import PReLU, LeakyReLU, Conv2D, Lambda, MaxPooling2D\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "import pickle\n",
    "import sklearn as skl\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa178fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some useful functions\n",
    "class PlotLossAccuracy(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.acc = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.val_acc = []\n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        self.logs.append(logs)\n",
    "        self.x.append(int(self.i))\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.acc.append(logs.get('acc'))\n",
    "        self.val_acc.append(logs.get('val_acc'))\n",
    "        \n",
    "        self.i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(16, 6))\n",
    "        plt.plot([1, 2])\n",
    "        plt.subplot(121) \n",
    "        plt.plot(self.x, self.losses, label=\"train loss\")\n",
    "        plt.plot(self.x, self.val_losses, label=\"validation loss\")\n",
    "        plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.title('Model Loss')\n",
    "        plt.legend()\n",
    "        plt.subplot(122)         \n",
    "        plt.plot(self.x, self.acc, label=\"training accuracy\")\n",
    "        plt.plot(self.x, self.val_acc, label=\"validation accuracy\")\n",
    "        plt.legend()\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        plt.show();\n",
    "        \n",
    "def save_model_to_disk():    \n",
    "    # save model and weights (don't change the filenames)\n",
    "    model_json = model.to_json()\n",
    "    with open(\"model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"model.h5\")\n",
    "    print(\"Saved model to model.json and weights to model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f26ba1",
   "metadata": {},
   "source": [
    "# Loading the new dataset (cifar100)\n",
    "\n",
    "We are going to use the 10 first classes of the cifar100 dataset.\n",
    "https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "> This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a \"fine\" label (the class to which it belongs) and a \"coarse\" label (the superclass to which it belongs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15120ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl --create-dirs -o /home/tcd/data/cifar100-dataset.pkl http://mindfreeze.ml/~mindfreeze/cifar100-dataset.pkl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a81de8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading new dataset cifar100\n",
    "\n",
    "print('loading the dataset...')\n",
    "\n",
    "pkl_file = open('/home/tcd/data/cifar100-dataset.pkl', 'rb')\n",
    "dataset = pickle.load(pkl_file)\n",
    "\n",
    "print('loaded.')\n",
    "\n",
    "print('let\\'s look at some of the pictures and their ground truth labels:')\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.plot([3, 3])\n",
    "\n",
    "X = dataset['X'].astype('float32')/255\n",
    "Y = dataset['Y'].astype('float32')\n",
    "Y = keras.utils.to_categorical(Y)\n",
    "\n",
    "for i in range(0,9):\n",
    "    # pictures are 32x32x3 (width=32, height=32, 3 colour channels)\n",
    "    pic = X[i]\n",
    "\n",
    "    # Y[i] returns an array of zeros and with Y[i][classid] = 1\n",
    "    # for instance  Y[i] = [ 0 0 0 0 0 1 0 0 0 0] => classid=5 \n",
    "    #          and  Y[i] = [ 1 0 0 0 0 0 0 0 0 0] => classid=0\n",
    "    # we can get the classid by using the argmax function on the vector Y[i]\n",
    "    classid = Y[i].argmax(-1)\n",
    "\n",
    "    # getting back the name of the label for that classid\n",
    "    classname = dataset['labels'][classid]\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(pic)\n",
    "    plt.title('label: {}'.format(classname))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26757587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training / validation split\n",
    "X_train, X_validation, Y_train, Y_validation = skl.model_selection.train_test_split(X, Y, test_size=.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8da1ce8",
   "metadata": {},
   "source": [
    "## Loading the Base Network\n",
    "\n",
    "Our starting point will be to take your CNN network from Lab 5 \n",
    "and use it for building visual features for the images of our new dataset.\n",
    "So let's start with re-loading your network. \n",
    "\n",
    "You don't need to retrain your network but you can if you want. If you do, note that Also, \n",
    "note that we probably don't need a very long training. The idea of this lab \n",
    "is only to show that you can get a good kick start from lab 5's network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81df3a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we assume that you have already successfully completed Lab 5 \n",
    "# and trained a CNN on cifar10\n",
    "\n",
    "# load json and create base_model\n",
    "with open('../lab-05/model.json', 'r') as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "base_model = model_from_json(loaded_model_json)\n",
    "# load weights into base_model\n",
    "base_model.load_weights(\"../lab-05/model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6859c29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the model summary\n",
    "base_model.summary()\n",
    "\n",
    "# In the summary, take note of the Layer names in the first column\n",
    "# eg. conv2d_6 (Conv2D) means that 'conv2d_6' will be the name for \n",
    "# that layer\n",
    "\n",
    "# In preparation for the next step, You also need to check that \n",
    "# shape of the Flatten layer is small enough. Typically we want something\n",
    "# like (None, 2048). You don't want (None, 40000)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c3cd40",
   "metadata": {},
   "source": [
    "# Part 1: Using the Base Network to Build Visual Features\n",
    "\n",
    "In this first part we are going to directly use the base model to pre-compute \n",
    "visual features on this new dataset. For this, we are going to compute the predictions \n",
    "for the intermediate `Flatten` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b91965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the `flatten_2` name to the corresponding name from your network\n",
    "\n",
    "features_model = keras.Model(inputs=base_model.input,\n",
    "                    outputs=base_model.get_layer('flatten_2').output)\n",
    "train_features = features_model.predict(X_train)\n",
    "validation_features = features_model.predict(X_validation)\n",
    "\n",
    "# checking that everything is ok, you should get a tensor of size 4500 x Flatten Size\n",
    "# eg. (4500, 2048)\n",
    "print(train_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3103c96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that you have computed the new features for your dataset,\n",
    "# you need to design a simple classification network.\n",
    "# For good measure, use one with BatchNormalization, dropout, etc. \n",
    "\n",
    "inputs = keras.layers.Input(shape=train_features.shape[1:]) \n",
    "# TODO: complete the network ....\n",
    "x = Dense(512, activation = 'relu')(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(256, activation = 'relu')(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "#x = Dropout(0.2)(x)\n",
    "\n",
    "predictions = Dense(10, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c38206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can compile it and train it.\n",
    "\n",
    "top_model = keras.models.Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "top_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "top_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79edf54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: write code to train it ...\n",
    "\n",
    "\n",
    "# the dataset is very small and the base_model gives us a very good \n",
    "# starting point, so we only need a handful of epochs, probably 1 \n",
    "# or 2 will do.\n",
    "\n",
    "# top_model.fit( ... )\n",
    "\n",
    "num_epochs = 4\n",
    "\n",
    "# Create an instance of our callback functions class, to plot our loss function and accuracy with each epoch.\n",
    "pltCallBack = PlotLossAccuracy()\n",
    "\n",
    "# Run the training.\n",
    "top_model.fit(train_features, Y_train,\n",
    "          batch_size=4096, epochs=num_epochs, \n",
    "          validation_data=(validation_features, Y_validation), \n",
    "          callbacks=[pltCallBack])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b16598",
   "metadata": {},
   "source": [
    "# Part 2: Fine-Tuning the Base Network\n",
    "\n",
    "In this part, instead of pre-computing the visual features, \n",
    "we are going to load the base model, freeze the weights for the convolutional layers and allow the optimizer to fine-tune the classification dense layers. This is done by setting the flag\n",
    "`trainable` to `True` or `False` in the network layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77449c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.clone_model(base_model)\n",
    "\n",
    "# set the all the layers in model to trainable = False\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "    \n",
    "model.layers[14].trainable = True\n",
    "model.layers[16].trainable = True\n",
    "model.layers[18].trainable = True\n",
    "model.layers[19].trainable = True\n",
    "\n",
    "for layer in model.layers:\n",
    "   print(layer, layer.trainable)\n",
    "\n",
    "# TODO: then set all the layers after the Flatten layer to be trainable    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6f21f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO compile model \n",
    "\n",
    "# model.compile(...)\n",
    "opt = keras.optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "# model.summary()\n",
    "model.summary()\n",
    "# loading the pre-trained weights from your lab-05 network\n",
    "model.load_weights(\"../lab-05/model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bd4319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO train 'model' on (X_train, Y_train) for 10 or so epochs  \n",
    "# What you should be able to observe is that the training is a bit slower than\n",
    "# in the previous case. This is partly because we haven't re-initialised the weights.\n",
    "num_epochs = 20\n",
    "\n",
    "# Create an instance of our callback functions class, to plot our loss function and accuracy with each epoch.\n",
    "pltCallBack = PlotLossAccuracy()\n",
    "\n",
    "# Run the training.\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=4096, epochs=num_epochs, \n",
    "          validation_data=(X_validation, Y_validation), \n",
    "          callbacks=[pltCallBack])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcb4560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write model to model.json and weights to model.h5 for submission\n",
    "\n",
    "save_model_to_disk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c0ba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the terminal, add model.json and model.h5 to git and submit the lab\n",
    "# Do the following: \n",
    "#  git add lab-06/model.json lab-06/model.h5\n",
    "#  git commit -m \"Added NN model.\"\n",
    "#  git push\n",
    "#  submit-lab 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baee0cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the terminal, add model.json and model.h5 to git and submit the lab\n",
    "# Do the following: \n",
    "#  git add lab-06/model.json lab-06/model.h5\n",
    "#  git commit -m \"Added NN model.\"\n",
    "#  git push\n",
    "#  submit-lab 6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}